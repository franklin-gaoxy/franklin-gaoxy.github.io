## 自我介绍
1. 工作年限 工作有三年多
2. 产品运维 提高产品稳定性 维护test UAT等环境 负责发布 CICD
3. 项目 对接客户 部署产品 数据备份 高可用部署 监控报警
## 简历所写
calico的Node-to-Node
etcd单独安装
自动部署脚本: shell 安装 python检查 连接到k8s集群获取所有pod和hosts的状态 调用关键服务接口进行检查



## Kubernetes

### 常见kubernetes网络组件

cilium(sei li e mu) flannel calico

### 常见端口
6443 api-server
2379 2380 etcd

### Kubernetes高可用
keepalived+haproxy 每个节点都运行api-server+etcd.最大支持n+1/2数量节点异常.
keepalived配置脚本请求api-server进行检查.
单独安装了etcd集群.

### Kubernetes支持存储
emptydir 空白目录 hostpath 主机路径 pv pvc storeclass Local卷 configMap secret
### Kubernetes支持探针
存活探针 启动探针 就绪探针
启动探针探测进程是否启动完成了 当他执行完成后才会执行就绪探针和存活探针 主要目的用于防止服务未启动成功则开始执行存活探针导致退出

 就绪探针探测服务是否准备好接收请求了，检查通过后才会将pod IP添加到service中。

### pod种类
初始化init容器 临时容器 业务容器 静态pod
### Kubernetes所有组件
etcd scheduler kube-apiserver controller-manager kubelet kube-proxy
插件: CNI(网络插件) CSI(存储接口) CRI(容器组件)
docker三核心：chroot(文件系统沙箱隔离) namespace（进程 用户 网络隔离） cgroups（资源限制）
### Kubernetes网络相关
calico node-to-node: calico会直接通过虚拟设备将容器网络接入到主机中。同时会将其他节点的网络区段记录在路由表中。当发送数据包时容器直接发送到主机内核，内核根据路由表决定这个数据包的”下一跳“地址，然后填写对端主机的IP地址。数据包到达对端主机后，会匹配当前主机的路由表直接找到虚拟设备，转发到容器中。
> 这个模式下会给每个主机划分一个网络区段.
> 当创建一个容器后,calico会直接创建一个虚拟网络设备,将容器直接连接到宿主机上.
> 然后再路由表添加一条记录,容器的IP地址要通过那个虚拟设备发送.
> 当要发送到其他节点数据时,数据包直接经过虚拟设备到达主机内核,内核根据路由表划分的区段,找到这个网络区段所在的主机节点,用他的IP当做下一跳的地址.然后发送出去.对端主机接收到后拆包,然后根据目的IP匹配路由表,找到对应容器的虚拟设备,发送到container.

calico默认的时IPIP模式，也就是将通过虚拟设备发送到主机内核后，内核会把这个包当成一个普通的数据包重新在他上面进行封装。优点是重新封装后的数据包和普通的网络数据包没什么不同，可以跨路由的传输，支持更大的集群架构。
#### calico和flannel的网络模式
calico: node-to-node IPIP host
flannel: host-gw VXLAN host
#### Pod请求Service的过程
首先,主机上会被创建一个kube-ipvs0的虚拟设备.这个虚拟设备,可以将请求转发到IPVS上.
接下来,创建一个service的时候,会自动创建service所对应的pod的IP组成的endpoints,并添加到ipvs对应的转发规则.
接下来Pod发起请求后,先访问了集群的DNS服务,解析获取到Service的ClusterIP.
然后对ClusterIP发起请求.但是需要注意的是,ClusterIP的地址和Pod的IP地址是两个不同的网段.这意味着他无法匹配到路由表中CNI配置的那些路由.所以会被直接通过kube-ipvs0这个虚拟设备发送.
IPVS接收到数据包后,他会找到对应的这个IP.这个IP对应了多个endpoint.他从里面选择一个endpoint的IP地址,然后对数据包进行DNAT(也就是修改目标IP,源IP不变.)而DNAT后,这个数据包要重新经过宿主机的内核发送到主机网络栈,匹配路由表,找到对端主机.
这样数据包会被重新发送出去.到达对端的Pod后,Pod可以根据源IP直接返回数据,不需要在经过IPVS.

### Kubernetes service种类
 ClusterIP ，Node Port ，headless，Load balancer
ingress
### Kubernetes相关

pod种类 deployment statefulset rs job cronjob pod daemonset
statefulset三大优点 稳定的存储 有序的扩缩容 稳定的网络标识
daemonset 每个节点运行一个pod 适合做日志收集
deployment 滚动升级和部署 控制副本数范围 同时还支持HPA
无状态和有状态:
> 无状态的服务不管启动多少份,请求哪一个副本,相应的内容都是相同的.
> 有状态的则不同,最明显的比如es集群,每个节点存储的数据不同.

### kubernetes的CNI CSI CRI

CSI： 通过grpc框架来通信

CNI：放在指定目录下，二进制文件方式调用，通过指定的参数来传递内容

CRI：通过Linux的sock文件来通信

### k8s pod的创建流程

> kubelet controller manager scheduler都是监视器 比如创建一个deployment
> kubectl 检查yaml格式，链接api-server 接下来是鉴权和认证
> api-server接受yaml 保存到etcd 然后deployment controller自动监控到 deployment会调用rs controller 创建对应数量的pod 
> 如果api-server检查pod发现直接指定了节点，那么不经过scheduler。如果没有指定，那么发送到scheduler。
> scheduler接收到需要调度pod 然后是 过滤-打分-绑定 三个步骤。过滤筛选不符合要求的，打分评选出一组最优的节点。
>
> shceduler开始调度首先检查集群内是否有可用节点，如果有则获取所有节点信息，调用过滤插件开始过滤节点，接下来检查过滤后的节点数量，如果为1则不再进行打分，否则则开始进行打分阶段。
>
> 如果过滤-打分 没有满足的节点，也就是调度失败了，那么可能会 触发 驱逐-抢占。之所以是可能 是因为要先对失败进行一次检查 如果是挂载卷不存在等问题不会触发。
> 驱逐抢占会先模拟一次整个过程，然后选择一个尽可能的驱逐优先级最低 尽可能少的Pod 方案，然后开始执行驱逐抢占操作
> 最后kubelet 监控到自己这个节点多了pod 开始调用cri 创建。

> 如果同时存在两个pod需要驱逐-抢占的过程，那么模拟过程会执行两遍。第一遍是正常的驱逐抢占过程，第二遍是存在竞争者的驱逐抢占过程。


```
kubectl开始检查yaml文件格式是否正确,关键字是否为空等.

使用"config"文件的证书内容,创建一个https请求,访问api-server.

api-server首先对提交的内容首先进行"认证",是否是加密的,所带的token是否正确.

接下来"鉴权",鉴权通过后会开始检查所提交的内容是否符合集群规则和限制.

接下来由api-server将提交信息持久化到etcd数据库.最后在发起一个get请求查看是否在数据库中了.

接下来会是controller来对他进行检查(这个步骤叫做 Initializer),负责在这个资源对外可见之前在执行一次处理,给pod注入证书,暴露端口或者打上特定的annotation(标注).

此步骤也可以被跳过,pod直接对外可见,如果没有跳过则在执行完毕之后对外可见.

初始化之后这个资源对外就是可见的(api-server会去更新他),然后会被对应资源的controller监控到,如deployment controller,然后他开始创建新的资源

k8s中存在大量的controller,他们不断地检查pod当前状态和期望状态是否一致,不一致则将其改变为期望状态,而这一组controller的集合就是Kubernetes中的 controller-manager.

deployment controller会通过api-server去查询调用replicas controller,如果没有对应资源则创建.如果有对应资源但是状态不一致则触发同步.

如果不存在,创建一个replicaSet然后分配一个标签.ReplicaSet的部分metadata则是来源于Deployment的metadata字段.

Deployment只负责创建或者同步ReplicaSet controller的状态,所以pod还是要ReplicaSet controller来创建,这也是deployment资源依赖ReplicaSet资源的原因.

ReplicaSet被创建之后.ReplicaSet controller会不断地检查他们的状态,发现不一致.开始增加pod数量.

创建的时候会按批次一点一点来创建,如果上一批成功那么下一批就翻倍.(为了不给api-server造成太大压力)

接下来就是其他的controller同步信息,清理缓存释放连接.这会etcd已经存储了一个deployment,一个replicaSet和对应数量的pod.

前面处理的都是"元数据"信息,主要是各个controller之间互相同步调用,pod此时可以被查到但处于Pending状态,接下来的scheduler才是创建pod的开始.

scheduler是独立于controller-manager存在的.他也是个监听器监听事件然后触发动作.

首先是predicates函数,也就是"过滤"节点.他会根据不同的规则:如指定了资源节点资源是否满足,是否设置了亲和性,来过滤出一组符合要求的节点.

接下来就是priority function,又称排序或者打分.这个步骤根据上一个步骤的结果在筛选出的node列表中,选取节点,如果指定了亲和性希望pod尽可能的分散开,那么则优先选择资源比较闲置的节点.

接下来对pod进行绑定到节点(binding),将pod和对应节点的关系记录,通知api-server.

api-server会将pod的nodeName指定上分配的节点的名字.添加annotations,最后设置pod PodScheduled状态为"true".表示调度成功.

接下来就是kubelet登场了.他也是一个controller,从api-server获取本节点需要存在的pod,然后和自己的缓存进行核对,发现不存在新建的则开始准备新建一个容器:如创建容器目录,cgroups,挂载卷,创建网络等.接下来调用CRI(容器)开始启动创建pod.
```
### k8s的驱逐和抢占
> 首先,Kubernetes有两个队列,一个叫做`active queue`,另一个是`unschedulable queue`.前者存放下一个调度周期要调度的对象,后者存放调度失败的对象.在平常情况下创建一个对象会先进入到`active queue`调度失败后会放进`unschedulable queue`队列,如果这个对象的配置文件被修改,那么则会重新放进`active queue`.
> 但是如果是抢占式的,调度失败同样被放进`unschedulable queue`中,但是会触发调度器为抢占者寻找牺牲者的一个动作.
> 第一,首先检查调度失败的原因.如果是容器指定了所需要的卷或者某种资源但是集群中不存在,删除再多的pod也是徒劳的.但是如果是集群资源不足的情况,那么则是可以允许发生抢占的.
> 第二,如果抢占可以发生,那么调度器把自己缓存的节点信息复制一份,用这个副本来模拟抢占过程.
> 模拟会产生多种结果,Kubernetes自然会选择牺牲最小的那个,也就是尽可能的少删除某个的pod,同时又尽量去驱逐那些优先级低的pod.
> 当获取到一个最优的结果后,就可以开始抢占的过程了,首先调度器开始检查牺牲者列表,清理他们所携带的`nominatedNodeName`字段,然后给抢占者添加`nominatedNodeName`字段为该节点的名称,标记这个抢占者未来可能去的位置,最后,开启一个Goroutine同步删除被驱逐的pod.
> 如果在抢占的过程中发现还有一个pod正在等待改节点驱逐其他pod,那么他会对于复制出来的节点信息的副本执行两遍模拟.
> 第一遍模拟另一个抢占者已经在这个节点运行的情况,第二遍正常的执行模拟而不考虑任何竞争者的存在.

#### 抢占的重要事项
> 1. 被驱逐的节点不会立刻被杀死,而是会获得30秒的"体面时间".类似于Linux的kill他会发送一个结束信号给到对应的pod自动调用一个退出函数,pod中的进程可以在这段时间保存数据打印停止日志等最后"体面"的结束
> 2. 如果被驱逐的pod超出了等待时间,他们则会被强制杀死.
> 3. 节点的pod在驱逐期间kube-scheduler不会持续等待,而是会继续调度其他服务.等到节点pod驱逐完成后会重新尝试调度该pod.
> 4. 在驱逐其他pod等待的期间,Kubernetes会给等待调度的新的pod打上一个`nominatedNodeName`的标签,值为接下来他要调度到的这个节点的名称.这样就可以跟踪这个pod的状态了.
> 5. 如果在pod等待节点驱逐其他pod的时候另一个节点可用了,那么该pod有可能会被调度到其他节点.同时等待的过程中如果提交了新的pod并且优先级比此pod要更高,那么被腾空的节点会用来运行这个新的pod.
> 6. 抢占过程中会优先删除优先级低的pod,如果该节点的pod被指定了亲和性或者PDB(pod干扰和抗干扰),那么则不会优先驱逐此pod.
> 7. 除了优先级低的,Kubernetes还会检查是否有pod使用的资源超出了limits(限制的资源),如果有,那么也驱逐当前pod.如果没有超出limits限制资源的,则会对比资源使用量.

### k8s的常见问题
#### 节点NotReady
可能原因: 1.节点的CRI异常 2.节点资源使用异常 3.节点被手动划为了维护模式 4. 节点系统存在问题 5.CRI存在问题 6. 节点驱逐或者节点资源预留.
#### 什么情况负载会极高?
节点IO过高的时候.
#### 亲和性调度失败
注意点: 亲和性是硬策略还是软策略?亲和性对象是节点还是其他POD?yaml文件是否指定了运行阶段如nodename字段?

1. 节点资源是否不足?
2. 如果亲和性对象是POD,那么那个Pod是否被创建了?
3. 节点是否具备相同标签?

### 静态Pod
kubeadm安装的服务,etcd,api-server等服务全部都是静态Pod启动的.
静态Pod是直接归属于kubelet管理的.kubelet是直接安装于主机的服务,所以启动的时候kubelet率先启动.kubelet启动成功以后,再拉起所有的静态Pod.而启动的这些静态Pod就是k8s的服务了.所以接下来,k8s集群启动完成,拉起集群中剩余的所有Pod.

### 其他资源种类

1. Pod开销
2. 亲和性和反亲和性（同时又分为应策略和软策略）加上节点污点和容忍度
3. namespace资源限制。（如果namespace指定了资源限制，那么创建的Pod也必须要指定。
4. QOS。（Pod中每个都指定了limit。只指定了request。没有指定任何限制。）驱逐和调度的时候都会按照这个。
5. 容器启动和停止函数。
### 存储
#### ceph 节点组件
monitor: 监控集群各个组件的状态,处理客户端元数据请求.
OSD: 负责存储和写入 读取数据
MGR: 管理组件,可以启动和停止组件.实现高可用.

> Monitor主要负责集群的监视和一致性管理，而Ceph Manager则提供集群的管理和监控接口

mds: 元数据存储,文件的增删改查,文件系统的访问
#### 高可用架构
三个monitor节点,三个OSD节点,三个MDS节点 ,三个manager(mgr)节点.
## Prometheus相关
### Prometheus组件
Prometheus server (时间序列数据库)
node_exporter(收集节点信息)： 其他包括JMX exporter,mysql exporter。主要用于收集对应组件的信息。
push gateway（自定义监控数据）
AlertManager （报警发送组件）

##### Kubernetes相关
Prometheus Adapter : HPA自动缩放相关
kube-state-metrics(mai tuai ke si)
cAdvisor(ai de fai se)
Prometheus operator （a po rui te）: 管理运行在Kubernetes上的Prometheus server等容器。


## Jenkins+gitlab
执行步骤: webhook触发Jenkins,Jenkins去gitlab拉取流水线文件.
接下来执行流水线文件,里面通过podTemplate定义了多个不同的容器,在容器里执行这些构建操作,通过stage定义了多个步骤.每个stage中又通过container定义了要在哪个容器中执行.
步骤包括 拉取代码 编译代码 构建镜像 推送镜像 更新服务 静态代码分析(SonarQuber)

## MySQL
### mysql写入流程

1. 首先开启一个事务
2. 所有的操作都会被记录在redo缓冲区中,并不会直接修改数据页
3. commit后,所有的缓冲区数据会写入到redo中,并且修改实际的表中的数据.
### MySQL简单语句

```sql
-- 查询
select * from database.table;
-- 修改
update table set cloume = "new key" where cloume = "value";
-- 插入
insert into table values (key...);
-- or 
insert into table (cloume...) values (key...);
-- 删除
delete from table where cloume = "key"
```

### tcpdump

```shell
tcpdump src [源地址] dst [目标地址] 
tcpdump host [ip地址]
tcpdump net [网段,ip/掩码格式]
tcpdump -i [网卡名]
```

```shell
# 根据逗号取倒数第二列
awk -F',' '{print $(NF - 1)}' test.txt
# grep -B 指定内容上面几行 -A 下面几行
grep "2" test.txt -B 1 -A 1
```

### 常用命令

```
ELK原理，L的没有有用过，自己编写过规则

TCP 三次握手和4次挥手过程

syn 置位1
ack置位1

FIN请求断开
回复FIN 回复ACK
客户端回复ACK

用一句话  kill 一个tomcat 进程 ps -f | grep tomcat | awk  '{print $x}' | xargs kill 


还有查找 一个文档里   含有  ERROR 字符的行和上下10行

grep "ERROR" -A 10 -B 10 txt

怎么查看定位CPU最高或者内存使用最高的进程top  -c   top  -m  


mysql  备份 和 回复 命令
mysqldump -u liulaoliu -p'MyNewPass4!' -h 127.0.0.1 --all-databases >a
mysqldump -u -p'xxx' -h aaa >a.txt
mysql -u -p -h databasename <a.txt

k8s  执行了一个ymal  服务没起来 怎么排查 哪里可以看到没起来的原因。。
kubectl get pod 查看状态
kubectl dscribe pod pod-name 查看描述

抓包命令 
tcpdump src [源地址] dst [目标地址] 
tcpdump host [ip地址]
tcpdump net [网段,ip/掩码格式]
tcpdump -i [网卡名]

如果有10台机器 用循环的方式 收集当前磁盘信息 
ssh-copy-id
ip=("111" "222")
for i in ip ;do 
ssh $i "df -h"
;done

介绍一个近期处理过得操作系统的故障。你从中有什么收获

Hadoop启动时 namenode异常 提示内存溢出 然后重新启动 监视日志 发现读取了大量的edits日志
增加内存让Hadoop暂时启动成功,然后修复secondary name node ,secondaryNamenode能够启动成功,但是同步的时候报错,原因为集群ID和namenode不同了,修改ID重启了secondaryNamenode 合并完成然后修改了Hadoop的jvm内存

MySQL主从原理
主库开启binlog,当有修改时会修改数据同时在binlog记录下来.
从库连接主库,同步binlog复制到自己本地.
重新执行binlog内容,将自己的数据和binlog数据同步.


写过哪些脚本，有什么作用

一键部署脚本 修改配置文件 检环环境 自动安装k8s集群 Hadoop集群 es集群 发布服务 等待启动 检查服务

LVS\KEEPALIVE 的应用场景 和 原理
yum安装 配置集群id 配置其他节点IP 配置每个节点的权重值 网卡名称 配置一个虚拟IP 也可以配置scripts执行脚本进行检查,检测的是进程退出状态码
生成虚拟IP 从节点么隔一段时间请求一次主节点 如果没有收到相应那么则自己绑定虚拟IP,多个从节点根据权重值来选举
```

### sql语句

SQL语句：左连接，分组，排序 语句怎么写。

```sql
-- 笛卡尔积 没看懂
select * from a cross join b;
-- 内连接 只显示符合要求的 也就是左边和右边的都相同
select * from a inner join b on a.id = b.id ;
-- 左连接 以左边的表为主键 匹配右边存在对应列的内容 没有则显示空 如第一个以a表为主键 第二个以b表为主键
select * from a left join b on a.id = b.id ;
select * from b left join a on a.id = b.id ;
-- 右连接 以右边的表为主键
select * from a right join b on a.id = b.id;
-- 分组
select * from a group by age,id;
-- 排序 数字从小到大 字母则从a-z 都有则优先数字
select * from a order by age ;
-- 逆序 从大到小
select * from a order by age desc;
```

其他sql

```sql
alter table city add index idx_name(name);
create index idx_name1 on city(name);
```

```
max() ：最大值
min() ：最小值
avg() ：平均值
sum() ：总和
count() ：个数
例子4：统计中国每个省的总人口数，将总人口数小于100w
SELECT district,SUM(Population)
FROM city
WHERE countrycode='chn'
GROUP BY district
HAVING SUM(Population) < 1000000 ;

去重
SELECT countrycode FROM city ;
SELECT DISTINCT(countrycode) FROM city ;
```

## Hadoop相关

### 所有组件

```
journalnode (zhe no no de)

namenode datanode secondaryNamenode
resourcemanager nodemanager jobHistoryserver
hmaster zk regionserver
```
> zkfc组件监控active name node 的运行状态。同时链接zookeeper，在znode中填写active NameNode相关信息。
> 出现异常的时候（active zkfc 监控到或者直接异常），standby(备用)节点切换为主节点。并且通知NameNode切换为active。
> zkfc监控Namenode状态,同时连接zookeeper.如果active Namenode宕机则汇报,同时选举一个新的Namenode.对应Namenode的zkfc会修改自己这个Namenode节点为active状态.
> 
> 在HA模式下，Quorum journal Manager (QJM)负责同步edits日志。他通过多个JournalNode 来组成集群保证高可用。
> edits日志的同步过程: active NameNode 将日志发送到QJM.QJM将接收到的edits日志,发送到所有其他节点的JournalNode上.然后接下来由所在节点的JournalNode 写入到当前主机.
> 同时JournalNode自己组成一个集群.同样采用半数机制


所以,ZKCF负责的是集群active NameNode的状态监控,如果异常选举新的NameNode.
而JournalNode配合QJM实现Hadoop NameNode的Edits日志同步.



[https://blog.csdn.net/weixin_43854618/article/details/108808274](https://blog.csdn.net/weixin_43854618/article/details/108808274)

## linux基础
#### 常用命令
```shell
# 取出某一列
awk -F'.' '{print $3}'
# 取出倒数第2列
awk -F'.' '{print $(NF - 2)}'
# 替换文本
sed -i 's@asd@asd@g' filename
# alias 设置别名
alise ll='ls -l'

# k8s相关
kubectl get node
kubectl get pod
kubectl describe pod podname
# 升级或者回滚
kubectl set image deployment web nginx=nginx:1.15
kubectl set image deployment podname containerName=image:tag
# 查看状态
kubectl rollout status deployment podName
# 回滚
kubectl rollout undo deployment web --to-revision=2
# 扩缩
kubectl scale --replicas=10 deployment deplName
# 重启
kubectl rollout restart deployment deplName
```
#### 权限相关

```
-rw-r--r-- 文件还是目录 属主权限 属组权限 其他人权限
r :4 w:2 x:1
```

## 高可用方案
### MySQL高可用方案
主从搭建: 首先开启binlog,然后同步库和binlog到从库,接下来从库执行语句链接主库,然后start slave开启,最后show slave status;查看状态.

1. 搭建主从服务器，配置好主从同步。然后利用keepalived生成虚拟IP地址，主服务器异常切换到从服务器。
   1. show slave status;查看状态。slave_io 和slave_sql 两项都为yes 表示关系正常。
2. MHA。MHA是单独的组件，他也要求mysql自己组成主从关系，相比之下他的功能在于，在三个节点的集群中，主节点异常，可以自动选择从节点将他设置为主节点。然后自动配置之前的从节点连接到新的主节点。他也支持keepalived。
### mysql主从原理
主库开启binlog,从库执行语句和主库建立连接.
主库会有一个单独的线程,将binlog日志发送到从节点,
从节点有两个线程 IO 线程和SQL线程.
IO线程接收主节点的binlog 写入到中继日志 SQL线程读取中继日志 然后执行内容.

### Redis主从

1. 主从模式。从节点从主节点同步数据，利用keepalived产生虚拟IP，然后主节点异常切换。
2. 哨兵。在主从模式中新增一个哨兵节点。他会监控主节点状态，异常后自动选择从节点提升为主节点。同时哨兵也支持集群。
3. 集群。多个主机组成一个集群，根据hash环分配数据，每个数据存储多个副本，不区分主从节点。
#### Redis的RDB和AOF
RDB: 内存快照,速度快体积小,但是可能会丢失部分数据.
AOF: Redis将所有的写操作以追加写的形式写入到文件,缺点是文件大速度慢,重启后会重新执行文件恢复,优点是不容易丢失数据.

##### RDB原理
linux下进程有一个fork的东西,他可以克隆出一份进程,两个进程的内存数据空间相同.接下来fork出来的这个进程将内存空间数据快照落盘.
#### Redis的读写流程
写入: Redis先把操作和数据发送到Redis的队列,Redis从队列中依次取出写入到内存,如果开启了持久化还会写入到磁盘
读取: client发送读取请求,Redis在内存中检查是否存在,不存在可能会读取磁盘文件(RDB和AOF),返回给client.
## 写过的shell脚本
数据备份,安装服务(python nginx mysql) 
产品部署(安装基础组件 Hadoop HBASE mysql k8s es)

### python脚本
服务监控检查,产品部署完成后的自动化检查(调用接口),链接k8s集群获取Pod hosts状态.
链接Hadoop集群,获取集群信息,运行的任务,剩余的资源,已用的资源 所有节点的组件状态...



